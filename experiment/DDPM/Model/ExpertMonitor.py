import torch
import torch.nn.functional as F
from collections import defaultdict
from torch.utils.tensorboard import SummaryWriter

from experiment.DDPM.Model.UNet import TimeAwareCondConv2d

class ExpertMonitor:
    def __init__(self, model, log_dir="runs/ddpm_experts"):
        self.hooks = []
        self.expert_stats = defaultdict(list)
        self.layer_names = []
        # åˆå§‹åŒ– TensorBoard Writer
        self.writer = SummaryWriter(log_dir)
        self._register_hooks(model)
        print(f"ğŸ‘€ ç›‘æ§å™¨å·²å¯åŠ¨ï¼æ—¥å¿—å°†ä¿å­˜åˆ°: {log_dir}")

    def _register_hooks(self, model):
        """
        è‡ªåŠ¨éå†æ¨¡å‹ï¼Œæ‰¾åˆ°æ‰€æœ‰çš„ TimeAwareCondConv2dï¼Œå¹¶ç‹ ç‹ åœ°æŒ‚ä¸Šé’©å­ã€‚
        """
        # éå†æ‰€æœ‰æ¨¡å—ï¼Œå¯»æ‰¾ç›®æ ‡å±‚
        for name, module in model.named_modules():
            # æˆ‘ä»¬åªå…³å¿ƒ TimeAwareCondConv2d
            if isinstance(module, TimeAwareCondConv2d):
                # æ³¨å†Œ Hook
                hook = module.router.register_forward_hook(
                    self._get_hook_fn(name)
                )
                self.hooks.append(hook)
                self.layer_names.append(name)
        print(f"å…±ç›‘æ§äº† {len(self.hooks)} ä¸ª TimeAwareCondConv2d å±‚")

    def _get_hook_fn(self, layer_name):
        """
        ç”Ÿæˆé—­åŒ…é’©å­å‡½æ•°ï¼Œä¸ºäº†è®°ä½æ˜¯å“ªä¸€å±‚çš„åå­—ã€‚
        """

        def hook(module, input, output):
            # output: (B, num_experts) -> Logits
            with torch.no_grad():
                # è®¡ç®— Softmax å¾—åˆ°æ¦‚ç‡
                probs = F.softmax(output, dim=1)
                # è®¡ç®—å½“å‰ Batch çš„å¹³å‡ä½¿ç”¨ç‡
                avg_usage = probs.mean(dim=0).detach().cpu()
                self.expert_stats[layer_name] = avg_usage

        return hook

    def log_step(self, global_step):
        """
        å°†å½“å‰è¿™ä¸€æ­¥çš„æ•°æ®å†™å…¥ TensorBoard
        """
        for layer_name, usage in self.expert_stats.items():
            # usage æ˜¯ä¸€ä¸ªå‘é‡ï¼Œä¾‹å¦‚ [0.25, 0.25, 0.25, 0.25]
            # æˆ‘ä»¬æŠŠå®ƒæ‹†å¼€è®°å½•ï¼Œè¿™æ ·ä½ å¯ä»¥çœ‹åˆ°æ¯ä¸ªä¸“å®¶çš„æ›²çº¿

            # è®°å½•æ¯ä¸ªä¸“å®¶çš„æ›²çº¿
            for i, u in enumerate(usage):
                self.writer.add_scalar(f"Expert_Usage/{layer_name}/Exp_{i}", u, global_step)

            # è®°å½•ç†µ (Entropy)
            # ç†µè¶Šé«˜(è¶Šæ¥è¿‘æœ€å¤§å€¼)ï¼Œè¯´æ˜è´Ÿè½½è¶Šå‡è¡¡ï¼›ç†µè¶Šä½ï¼Œè¯´æ˜ä¸“å®¶åå¡Œäº†
            # H = -sum(p * log(p))
            # åŠ ä¸Š 1e-9 é˜²æ­¢ log(0)
            entropy = -torch.sum(usage * torch.log(usage + 1e-9))
            self.writer.add_scalar(f"Expert_Entropy/{layer_name}", entropy, global_step)

    def print_summary_to_console(self, tqdm_bar=None):
        """
        å¦‚æœä½ éè¦çœ‹æ§åˆ¶å°ï¼Œç”¨è¿™ä¸ªæ–¹æ³•ã€‚
        å®ƒä¼šä½¿ç”¨ tqdm.write é¿å…æ‰“æ–­è¿›åº¦æ¡ã€‚
        """
        msg = "\nğŸ“Š [Expert Monitor Snapshot]\n"
        for name in self.layer_names:
            if name in self.expert_stats:
                u = self.expert_stats[name]
                # æ ¼å¼åŒ–ä¸€ä¸‹ï¼Œæ¯”å¦‚ [0.25, 0.25, 0.25, 0.25]
                u_str = " | ".join([f"{x:.2f}" for x in u])
                msg += f"  {name[-20:]:<20}: [{u_str}]\n"

        if tqdm_bar:
            tqdm_bar.write(msg)
        else:
            print(msg)

    def close(self):
        for h in self.hooks:
            h.remove()
        self.writer.close()