import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.utils import save_image
from tqdm import tqdm

from experiment.DDPM.DDPM import DDPM
from experiment.DDPM.Tools.ExpertMonitor import ExpertMonitor


def run_training(
        model_cls,
        dataset,
        epochs=200,
        batch_size=64,
        accumulation_steps=4,  # ç´¯ç§¯å‡ æ­¥æ›´æ–°ä¸€æ¬¡
        lr=2e-4,
        device="cuda",
        save_path="ddpm_cat.pth",
        use_monitor=False,
):
    # 1. åˆå§‹åŒ–æ¨¡å‹å’Œ DDPM
    unet = model_cls()
    ddpm = DDPM(model=unet, timesteps=1000).to(device)

    # 2. æ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

    # 3. ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.AdamW(ddpm.parameters(), lr=lr, weight_decay=0)

    # ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼ŒT_max è®¾ä¸ºæ€»æ­¥æ•°æˆ– Epoch æ•°
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)
    if use_monitor:
        monitor = ExpertMonitor(unet, log_dir=r"runs/ddpm_experiment")
    # 4. è®­ç»ƒå¾ªç¯
    batch_loss = 0
    accum_counter = 0
    global_step = 0

    print(f"ğŸš€ Start Training on {device}...")
    print(f"   Batch Size: {batch_size}, Accumulation: {accumulation_steps}")
    print(f"   Effective Batch Size: {batch_size * accumulation_steps}")

    for epoch in range(epochs):
        ddpm.train()
        epoch_loss = 0.0
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{epochs}")

        for step, batch in enumerate(progress_bar):
            # å‡è®¾ dataset è¿”å›çš„æ˜¯ (image, label) æˆ–è€… image
            # æˆ‘ä»¬åªéœ€è¦ image
            if isinstance(batch, (list, tuple)):
                images = batch[0]
            else:
                images = batch

            images = images.to(device)

            # é‡è¦ï¼šDDPM å‡è®¾è¾“å…¥åœ¨ [-1, 1]ï¼Œå¦‚æœ DataLoader è¾“å‡ºæ˜¯ [0, 1]ï¼Œéœ€è¦è½¬æ¢
            # images = images * 2.0 - 1.0

            # ================= 1. è®¡ç®—ä¸» Loss (ä¾èµ–æ•°æ®) =================
            loss_main = ddpm.compute_loss(images)

            # æ¢¯åº¦ç´¯ç§¯
            # é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼Œå› ä¸º backward ä¼šç´¯åŠ æ¢¯åº¦
            loss_main = loss_main / accumulation_steps
            # åå‘ä¼ æ’­ä¸» Loss
            loss_main.backward()

            # è¿˜åŸä¸» Loss æ•°å€¼ç”¨äºæ‰“å°
            loss_val = loss_main.item() * accumulation_steps
            epoch_loss += loss_val
            avg_loss = epoch_loss / len(dataloader)

            # ç”¨äºæ—¥å¿—è®°å½• (è¿˜åŸæ•°å€¼)
            batch_loss += loss_main.item()

            # è®¡æ•°å™¨ +1
            accum_counter += 1

            # åªæœ‰æ»¡è¶³ç´¯ç§¯æ­¥æ•°æ—¶æ‰è¿›è¡Œæ›´æ–°
            if accum_counter % accumulation_steps == 0:
                # æ­£åˆ™åŒ– Loss åªä¸æƒé‡æœ‰å…³ï¼Œä¸ Batch å¤§å°æ— å…³ï¼Œ
                # æ‰€ä»¥ä¸éœ€è¦é™¤ä»¥ accumulation_stepsï¼Œç›´æ¥åŠ ä¸€æ¬¡æ¢¯åº¦å³å¯ã€‚
                aux_loss = unet.get_auxiliary_loss()
                aux_loss.backward()

                # æ¢¯åº¦è£å‰ª (Max Norm é€šå¸¸è®¾ä¸º 1.0)
                torch.nn.utils.clip_grad_norm_(ddpm.parameters(), max_norm=1.0)

                optimizer.step()
                optimizer.zero_grad()
                global_step += 1

                if use_monitor:
                    # æ­¤æ—¶ monitor.batch_buffer é‡Œå·²ç»æ”’äº† N ä¸ª mini-batch çš„æ•°æ®
                    # è°ƒç”¨ log_and_reset è¿›è¡Œç»“ç®—ã€è®°å½•å¹¶æ¸…ç©º
                    monitor.log_and_reset(global_step)

                    # é¡ºä¾¿è®°å½•ä¸€ä¸‹ Loss å’Œ LR
                    monitor.writer.add_scalar("Train/Loss", batch_loss, global_step)
                    monitor.writer.add_scalar("Train/LR", optimizer.param_groups[0]['lr'], global_step)

                # é‡ç½®ç´¯ç§¯çš„ batch_loss
                batch_loss = 0

            progress_bar.set_postfix({"loss": f"{loss_val:.4f}", "lr": f"{optimizer.param_groups[0]['lr']:.6f}", "global step:": f"{global_step}"})

        # æ¯ä¸ª Epoch ç»“æŸåè°ƒæ•´å­¦ä¹ ç‡
        scheduler.step()

        print(f"ğŸ“‰ Epoch {epoch + 1} Average Loss: {avg_loss:.4f}")

        # æ¯éš” 10 ä¸ª epoch ä¿å­˜ä¸€æ¬¡ï¼Œå¹¶å°è¯•é‡‡æ ·çœ‹æ•ˆæœ
        # if (epoch + 1) % 10 == 0:
            # torch.save(ddpm.model.state_dict(), save_path)
            # print(f"ğŸ’¾ Model saved to {save_path}")

        # é‡‡æ ·å¹¶ä¿å­˜å›¾ç‰‡
        print("ğŸ¨ Sampling images...")
        generated_imgs = ddpm.sample(num_samples=16, img_size=64)
        save_image(generated_imgs, f"output_epoch_{epoch + 1}.png", nrow=4)
        print(f"âœ… Saved sample to output_epoch_{epoch + 1}.png")

    if use_monitor:
        monitor.close()
    print("âœ… Training Finished!")